{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image generation project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPR+z6XpwOMJhgSaUjM+Akj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mP87OPmtdkYR"},"source":["В этом канале будет происходить обсуждение проекта по генерации изображений. Вопросы лучше задавать сюда, а не мне в телеграмм, ведь у кого-то из вас могут возникать одинаковые вопросы, да и сами вопросы и ответы могут быть полезны другим.\r\n","\r\n","  По поводу того, что нужно сделать. Необходимо самому реализовать архитектуру CycleGAN или что-то подобное решающее ту же задачу. Код CycleGAN действительно есть в открытом доступе, но копировать его нельзя, ведь мы сами хотим изучить все в деталях. План действий примерно такой: сначала смотрим на материалы по ганам и понимаем, что там происходит, потом пытаемся сами его написать. Если возникают сложности с написанием, то можно подсмотреть в готовую реализацию, но не просто ее скопировать, а написать похожий код под свою кодовую базу. Если вы берете откуда-то код, например скрипт для загрузки датасета, то обязательно в комментариях укажите ссылку на оригинальный код.\r\n","\r\n","  После того, как вы реализовали модель и пайплайн обучение своими руками нужно убедиться, что все работает. Для этого мы берем уже известный датасет, который указан в статье (например monet2photo) и смотрим, что результаты обучения сильно не различаются от тех, что в статье (нет серьезных артефактов итд). Если возникают проблемы на этот счет (получаются странные результаты), то смело пишите сюда.\r\n","  \r\n","  Теперь можно пробовать решить свою задачу. Для этого необходимы всего два набора изображений. Тут можете брать любой перспективный на ваш взгляд вариант. Если вы не уверены в выборе, тоже пишите сюда.\r\n","\r\n","Оставлю пару ссылок, которые помогли мне, я считаю что вы уже знаете про GAN-ы, если нет, то пишите, я что-нибудь найду попроще, ну или сами почитайте).\r\n","\r\n","*   Начнем, наверное с теории. Почему-то мне понравилось это видео в свое время, там очень хорошо объяснено, почему в ганах используется именно такой лосс. В общем для тех, кто любит теорию вероятностей: \r\n","  https://youtu.be/7G4_Y5rsvi8 \r\n","*   Еще видео, теперь уже про unpaired image translation. Здесь рассказана основная идея моделей.\r\n","https://youtu.be/m47qsfSZoTI\r\n","*   Статья про lsgan. Такой лосс используется в CycleGAN. В ней предлагают использовать немного другой лосс. Он тоже минимизирует дивергенцию, но не Йенсена - Шеннона как в первом видео, а Пирсона. Результаты часто получаются чуть лучше при таком подходе. Не заморачивайтесь, если что-то не поняли. Это просто, чтобы вы не сильно удивлялись откуда там квадрат разности вместо логарифма в лоссе.\r\n","https://arxiv.org/pdf/1611.04076.pdf\r\n","*   Артефакт, который часто возникает при использовании ConvTransposed слоев.\r\n","https://distill.pub/2016/deconv-checkerboard/\r\n","*   В дополнение к предыдущему пункту\r\n","https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/190#issuecomment-358546675\r\n","\r\n","  По поводу интерфейса. Если хотите, то можете сделать, например, телеграмм бота или просто API, которое принимает изображение и делает из него нечто. Например можно использовать вот это: \r\n","https://github.com/catalyst-team/reaction\r\n","\r\n","Удачи!"]},{"cell_type":"code","metadata":{"id":"VA8fFroudk8D"},"source":[""],"execution_count":null,"outputs":[]}]}